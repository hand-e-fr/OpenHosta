{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hand-e-fr/OpenHosta/blob/3.0.0_beta1/OpenHosta/tree/3.0.0_beta1/docs/SmokeTest_ollama.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Instal OpenHosta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install --upgrade -qqq uv\n",
        "\n",
        "# If you need to test a pre release uncomment:\n",
        "VERSION=\"@3.0.0_beta1\"\n",
        "\n",
        "!uv pip install -U \\\n",
        "    \"git+https://github.com/hand-e-fr/OpenHosta.git$VERSION\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import OpenHosta; print(OpenHosta.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Install Ollama\n",
        "\n",
        "This is to run a local model and have zero dependancies to externa API provides."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PfTzrCGKc8eG",
        "outputId": "31287fb2-e5a7-4c3f-d6e9-96a32de27d5f"
      },
      "outputs": [],
      "source": [
        "# This seems not to be accepted by google colab anymore. \n",
        "# Run it in a separtated terminal i refused\n",
        "#!apt install -y screen\n",
        "#!curl -fsSL https://ollama.com/install.sh | sh\n",
        "#!screen -dmS ollama ollama serve\n",
        "\n",
        "# So we just show how to install Ollama on linux\n",
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We test is ollama is available and how fast Qwen3 runs on it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i1st6ZJpdGn0"
      },
      "outputs": [],
      "source": [
        "!ollama run qwen3:4b hello --verbose  2>&1 | grep -E \":\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Use OpenHosta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJzVlcvjbf38"
      },
      "outputs": [],
      "source": [
        "\n",
        "from OpenHosta import config\n",
        "\n",
        "# You can replace with your own API (OpenAI chat/completion compatble)\n",
        "config.DefaultModel.base_url = \"http://localhost:11434/v1\"\n",
        "config.DefaultModel.model_name = \"qwen3:4b\"\n",
        "config.DefaultModel.api_key = \"not used by ollama local api\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**We test that the API is working with a simple call**\n",
        "\n",
        "`ask()` makes a very simple call to the API without adding any hidden prompt.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from OpenHosta import ask\n",
        "\n",
        "ask(\"Hello World!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Most OpenHosta functions are avaiale in sync and async flavor.\n",
        "\n",
        "Async versions are provided by adding `_async` or by importing from  `from OpenHosta.asynchrone`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 609
        },
        "id": "Kmw7wRVhcLzm",
        "outputId": "e60b6684-afaf-4f9e-b94e-eed9b4fbb303"
      },
      "outputs": [],
      "source": [
        "from OpenHosta.asynchrone import ask\n",
        "\n",
        "# Another way to get the same result is:\n",
        "# from OpenHosta import ask_async as ask\n",
        "\n",
        "await ask(\"Hello World!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# The emulate function\n",
        "\n",
        "The main purpose of OpenHosta is to let you organize your code in python and let you decide later if you need to implement function bodies in native python or let an LLM do the job. \n",
        "\n",
        "This is really pythonic as types and doc strings will remain unchanged if you decide to replace `return emulate()` by your own code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oNEhOe1ebtQ1"
      },
      "outputs": [],
      "source": [
        "from OpenHosta import emulate\n",
        "\n",
        "def translate(text:str, language:str)->str:\n",
        "    \"\"\"\n",
        "    This function translates the text in the “text” parameter into the language specified in the “language” parameter.\n",
        "    \"\"\"\n",
        "    return emulate()\n",
        "\n",
        "result = translate(\"Hello World!\", \"French\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In very simple words, `emulate()` does the inspection of the function it is called from and buld a prompt that delegated computation of the function to an LLM.\n",
        "This is very usefull for NLP based functions, but also **smart** decisions in your workflow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from enum import Enum\n",
        "\n",
        "class Choice(Enum):\n",
        "    \"\"\"\n",
        "    WebSearch = \"Generic Subject, but not enough confidence in LLM knowledge\"\n",
        "    UseLLM    = \"Generic Subject, obviouse answer.\"\n",
        "    UserRAG   = \"Company private subject, need to search for internal documentation through RAG\"\n",
        "    \"\"\"\n",
        "    WebSearch = \"WebSearch\"\n",
        "    UseLLM    = \"UseLLM\"\n",
        "    UserRAG   = \"UserRAG\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from OpenHosta import emulate\n",
        "\n",
        "def decide_if_search_or_answer(query: str) -> Choice:\n",
        "    \"\"\"\n",
        "    Decide to search for answer elements in one of the multiple Choice locations.\n",
        "    \n",
        "    When the LLM is usure about the correct answer it shall query external data sources.\n",
        "    \n",
        "    Args:\n",
        "        subject (str): The question that we are working on\n",
        "    Return:\n",
        "        Choice: the best strategy to use\n",
        "    \"\"\"\n",
        "    return emulate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "decide_if_search_or_answer(\"april 2021 is after mars 2020?\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "decide_if_search_or_answer(\"who won the last NFL cup?\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "decide_if_search_or_answer(\"when was the first soccer world cup?\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Inspect LLM Dialog"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ok, but can I see the prompt behind and what was really sent to the API?\n",
        "-YES-\n",
        "\n",
        "And the thinking and answer of the LLM ?\n",
        "-YES-"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from OpenHosta import print_last_prompt\n",
        "\n",
        "print_last_prompt(decide_if_search_or_answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can also see and modify the prompt before it is sent to the LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from OpenHosta.core.meta_prompt import EMULATE_META_PROMPT, USER_CALL_META_PROMPT\n",
        "from OpenHosta import MetaPrompt\n",
        " \n",
        "config.DefaultPipeline.emulate_meta_prompt = MetaPrompt(EMULATE_META_PROMPT.source)\n",
        "config.DefaultPipeline.user_call_meta_prompt = MetaPrompt(USER_CALL_META_PROMPT.source)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Go further with agents\n",
        "\n",
        "There is another notebook that shows how to use agents and memory with OpenHosta.\n",
        "[Agent with Memory](Agent.ipynb)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNhsodrHnwmlJ7/0yYQTF1p",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "VSCode_GitRepos",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
