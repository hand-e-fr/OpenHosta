{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hand-e-fr/OpenHosta/blob/3.0.0_beta1/OpenHosta/tree/3.0.0_beta1/docs/SmokeTest_ollama.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Instal OpenHosta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2mUsing Python 3.12.3 environment at: /home/ebatt/VSCode_GitRepos/.venv\u001b[0m\n",
            "\u001b[2K\u001b[2mResolved \u001b[1m9 packages\u001b[0m \u001b[2min 151ms\u001b[0m\u001b[0m                                         \u001b[0m\n",
            "\u001b[2mAudited \u001b[1m9 packages\u001b[0m \u001b[2min 0.48ms\u001b[0m\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade -qqq uv\n",
        "\n",
        "# If you need to test a pre release uncomment:\n",
        "VERSION=\"@3.0.0_beta1\"\n",
        "\n",
        "!uv pip install -U \\\n",
        "    \"git+https://github.com/hand-e-fr/OpenHosta.git$VERSION\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3.0.0\n"
          ]
        }
      ],
      "source": [
        "import OpenHosta; print(OpenHosta.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Install Ollama\n",
        "\n",
        "This is to run a local model and have zero dependancies to externa API provides."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PfTzrCGKc8eG",
        "outputId": "31287fb2-e5a7-4c3f-d6e9-96a32de27d5f"
      },
      "outputs": [],
      "source": [
        "# This seems not to be accepted by google colab anymore. \n",
        "# Run it in a separtated terminal i refused\n",
        "#!apt install -y screen\n",
        "#!curl -fsSL https://ollama.com/install.sh | sh\n",
        "#!screen -dmS ollama ollama serve\n",
        "\n",
        "# So we just show how to install Ollama on linux\n",
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We test is ollama is available and how fast Qwen3 runs on it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "i1st6ZJpdGn0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total duration:       2.679660793s\n",
            "load duration:        2.002796383s\n",
            "prompt eval count:    9 token(s)\n",
            "prompt eval duration: 145.13192ms\n",
            "prompt eval rate:     62.01 tokens/s\n",
            "eval count:           116 token(s)\n",
            "eval duration:        529.879618ms\n",
            "eval rate:            218.92 tokens/s\n"
          ]
        }
      ],
      "source": [
        "!ollama run qwen3:4b hello --verbose  2>&1 | grep -E \":\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Use OpenHosta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "YJzVlcvjbf38"
      },
      "outputs": [],
      "source": [
        "\n",
        "from OpenHosta import config\n",
        "\n",
        "# You can replace with your own API (OpenAI chat/completion compatble)\n",
        "config.DefaultModel.base_url = \"http://localhost:11434/v1\"\n",
        "config.DefaultModel.model_name = \"qwen3:30b\"\n",
        "config.DefaultModel.api_key = \"not used by ollama local api\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**We test that the API is working with a simple call**\n",
        "\n",
        "`ask()` makes a very simple call to the API without adding any hidden prompt.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'<think>\\nOkay, the user said \"Hello World!\" which is a classic first program in many programming languages. Let me check what they might need.\\n\\nHmm, they might be a beginner in programming or just testing if I\\'m responding properly. Since it\\'s a common introductory message, they could be expecting a confirmation or an example response.\\n\\nI should respond in a friendly and helpful manner. Maybe provide a simple example of a \"Hello, World!\" program in a couple of programming languages to be helpful. But wait, the user might just want a simple greeting. Let me not overcomplicate it.\\n\\nWait, the user message is \"Hello World!\" with an exclamation mark, so maybe they\\'re just saying hello, not asking for code. So the right reply is a friendly greeting back. But I should check if they need anything specific.\\n\\nSince they mentioned \"Hello World!\", which is often a default test message, I\\'ll reply with a standard greeting and offer help if they need programming examples or anything else.\\n\\nLet me think of a response: \"Hello! üëã How can I assist you today? If you\\'re looking for a \\'Hello, World!\\' program example in any language, just let me know!\"\\n\\nThat\\'s friendly and provides options. I\\'ll go with that.\\n</think>\\n\\nHello! üëã How can I assist you today? If you\\'re looking for a \"Hello, World!\" program example in any programming language, just let me know! üòä'"
            ]
          },
          "execution_count": 129,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from OpenHosta import ask\n",
        "\n",
        "ask(\"Hello World!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Most OpenHosta functions are avaiale in sync and async flavor.\n",
        "\n",
        "Async versions are provided by adding `_async` or by importing from  `from OpenHosta.asynchrone`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 609
        },
        "id": "Kmw7wRVhcLzm",
        "outputId": "e60b6684-afaf-4f9e-b94e-eed9b4fbb303"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'<think>\\nOkay, the user sent \"Hello World!\" Maybe they\\'re testing if I\\'m working, or maybe it\\'s a typo. Let me check the history to see if there\\'s context. Oh, right, this is the first message. So I should respond politely.\\n\\nLet me think of a friendly, helpful reply. I should say hello back, maybe ask how I can assist them. Keep it simple and welcoming. No need to overcomplicate it. Make sure it\\'s grammatically correct and friendly. Let me draft something: \"Hello! How can I assist you today?\" Wait, maybe add an emoji to keep it warm. Yeah, like a smiley. So, \"Hello! üòä How can I assist you today?\" That sounds good. Let me make sure there\\'s no typo. Yep, looks good. Alright, that\\'s the response.\\n</think>\\n\\nHello! üòä How can I assist you today?'"
            ]
          },
          "execution_count": 130,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from OpenHosta.asynchrone import ask\n",
        "\n",
        "# Another way to get the same result is:\n",
        "# from OpenHosta import ask_async as ask\n",
        "\n",
        "await ask(\"Hello World!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# The emulate function\n",
        "\n",
        "The main purpose of OpenHosta is to let you organize your code in python and let you decide later if you need to implement function bodies in native python or let an LLM do the job. \n",
        "\n",
        "This is really pythonic as types and doc strings will remain unchanged if you decide to replace `return emulate()` by your own code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "id": "oNEhOe1ebtQ1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bonjour le monde!\n"
          ]
        }
      ],
      "source": [
        "from OpenHosta import emulate\n",
        "\n",
        "def translate(text:str, language:str)->str:\n",
        "    \"\"\"\n",
        "    This function translates the text in the ‚Äútext‚Äù parameter into the language specified in the ‚Äúlanguage‚Äù parameter.\n",
        "    \"\"\"\n",
        "    return emulate()\n",
        "\n",
        "result = translate(\"Hello World!\", \"French\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In very simple words, `emulate()` does the inspection of the function it is called from and buld a prompt that delegated computation of the function to an LLM.\n",
        "This is very usefull for NLP based functions, but also **smart** decisions in your workflow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {},
      "outputs": [],
      "source": [
        "from enum import Enum\n",
        "\n",
        "class Choice(Enum):\n",
        "    \"\"\"\n",
        "    WebSearch = \"Generic Subject, but not enough confidence in LLM knowledge\"\n",
        "    UseLLM    = \"Generic Subject, obviouse answer.\"\n",
        "    UserRAG   = \"Company private subject, need to search for internal documentation through RAG\"\n",
        "    \"\"\"\n",
        "    WebSearch = \"WebSearch\"\n",
        "    UseLLM    = \"UseLLM\"\n",
        "    UserRAG   = \"UserRAG\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {},
      "outputs": [],
      "source": [
        "from OpenHosta import emulate\n",
        "\n",
        "def decide_if_search_or_answer(query: str) -> Choice:\n",
        "    \"\"\"\n",
        "    Decide to search for answer elements in one of the multiple Choice locations.\n",
        "    \n",
        "    When the LLM is usure about the correct answer it shall query external data sources.\n",
        "    \n",
        "    Args:\n",
        "        subject (str): The question that we are working on\n",
        "    Return:\n",
        "        Choice: the best strategy to use\n",
        "    \"\"\"\n",
        "    return emulate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<Choice.UseLLM: 'UseLLM'>"
            ]
          },
          "execution_count": 134,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "decide_if_search_or_answer(\"april 2021 is after mars 2020?\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<Choice.UseLLM: 'UseLLM'>"
            ]
          },
          "execution_count": 135,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "decide_if_search_or_answer(\"who won the last NFL cup?\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<Choice.UseLLM: 'UseLLM'>"
            ]
          },
          "execution_count": 136,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "decide_if_search_or_answer(\"when was the first soccer world cup?\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ok, but can I see the prompt behind and what was really sent to the API?\n",
        "-YES-\n",
        "\n",
        "And the thinking and answer of the LLM ?\n",
        "-YES-"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "System prompt:\n",
            "-----------------\n",
            "You will act as a simulator for functions that cannot be implemented in actual code.\n",
            "\n",
            "I'll provide you with function definitions described in Python syntax. \n",
            "These functions will have no body and may even be impossible to implement in real code, \n",
            "so do not attempt to generate the implementation.\n",
            "\n",
            "Instead, imagine a realistic or reasonable output that matches the function description.\n",
            "I'll ask questions by directly writing out function calls as one would call them in Python.\n",
            "Respond with an appropriate return value, without adding any extra comments or explanations.\n",
            "If the provided information isn't enough to determine a clear answer, respond simply with \"None\".\n",
            "If assumptions need to be made, ensure they stay realistic, align with the provided description.\n",
            "\n",
            "Here's the function definition:\n",
            "\n",
            "```python\n",
            "# Python enum Choice definition.\n",
            "# When you return a Choice, print the enum member value as a string. I will identify the corresponding enum member.\n",
            "class Choice(Enum):\n",
            "    \"\"\"\n",
            "    WebSearch = \"Generic Subject, but not enough confidence in LLM knowledge\"\n",
            "    UseLLM    = \"Generic Subject, obviouse answer.\"\n",
            "    UserRAG   = \"Company private subject, need to search for internal documentation through RAG\"\n",
            "    \"\"\"\n",
            "    WebSearch = 'WebSearch'\n",
            "    UseLLM = 'UseLLM'\n",
            "    UserRAG = 'UserRAG'\n",
            "\n",
            "def decide_if_search_or_answer(query: str) -> Choice:\n",
            "    \"\"\"\n",
            "    Decide to search for answer elements in one of the multiple Choice locations.\n",
            "\n",
            "    When the LLM is usure about the correct answer it shall query external data sources.\n",
            "\n",
            "    Args:\n",
            "        subject (str): The question that we are working on\n",
            "    Return:\n",
            "        Choice: the best strategy to use\n",
            "    \"\"\"\n",
            "\n",
            "    ...\n",
            "    ...behavior to be simulated...\n",
            "    ...\n",
            "\n",
            "    return ...appropriate return value...\n",
            "```\n",
            "\n",
            "/no_think/no_think/no_think/no_think/think/no_think/think\n",
            "User prompt:\n",
            "-----------------\n",
            "# Values of parameters to be used\n",
            "query='when was the first soccer world cup?'\n",
            "\n",
            "decide_if_search_or_answer(query)\n",
            "LLM response:\n",
            "-----------------\n",
            "<think>\n",
            "Okay, let's see. I need to figure out what the function decide_if_search_or_answer should return when the query is \"when was the first soccer world cup?\".\n",
            "\n",
            "Looking at the Choice enum, there are three options: WebSearch, UseLLM, UserRAG. \n",
            "\n",
            "WebSearch is for when the LLM isn't confident and needs to look up external info. UseLLM is for generic subjects with obvious answers. UserRAG is for company-specific stuff needing internal docs.\n",
            "\n",
            "The question is about the first soccer World Cup. I know that the first FIFA World Cup was in 1930, but maybe the LLM can answer that without needing to search. Wait, but is that common knowledge? The LLM might know that because it's a well-known historical fact. So if it's a straightforward question with a known answer, the function would return UseLLM. \n",
            "\n",
            "Wait, the description says UseLLM is \"Generic Subject, obvious answer.\" Since the World Cup date is pretty standard, it's probably an obvious answer for the LLM. So the correct choice here would be UseLLM. \n",
            "\n",
            "Wait, but let me confirm. The first World Cup was in 1930 in Uruguay. If the LLM has been trained on that data, it should answer it directly. So the function would choose UseLLM, not WebSearch. WebSearch would be if it's unsure, like if the question is about something obscure. Since this is a common fact, UseLLM is the right choice.\n",
            "</think>\n",
            "\n",
            "UseLLM\n"
          ]
        }
      ],
      "source": [
        "from OpenHosta import print_last_prompt\n",
        "\n",
        "print_last_prompt(decide_if_search_or_answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Build a workflow agent\n",
        "\n",
        "Let's see how to impement an agent with memory within an object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "import asyncio\n",
        "from dataclasses import dataclass\n",
        "from OpenHosta.asynchrone import emulate, closure\n",
        "\n",
        "from enum import Enum\n",
        "\n",
        "class SubjectsInMemoryStore(Enum):\n",
        "    FamilyMemberLinks = \"FamilyMemberLinks\"\n",
        "    PeopleNames = \"PeopleNames\"\n",
        "    Grocery = \"Grocery\"\n",
        "    CalendarEvents = \"CalendarEvents\"\n",
        "    NotValidSubject = \"NotValidSubject\"\n",
        "    \n",
        "class ActionType(Enum):\n",
        "    INSERT=\"INSERT\"\n",
        "    UPDATE=\"UPDATE\"\n",
        "    DELETE=\"DELETE\"\n",
        "    SELECT=\"SELECT\"\n",
        "    \n",
        "    \n",
        "@dataclass\n",
        "class UserDetails:\n",
        "    FirstName: str\n",
        "    LastName: str\n",
        "    Age: int\n",
        "\n",
        "class MyAgentWithMemory:\n",
        "    \"\"\"\n",
        "    This agent records elements from the conversation only if they are related to specific subkects\n",
        "    \"\"\"\n",
        "    \n",
        "    CurrentUserId:str = None\n",
        "    MemoryElements:dict = {}\n",
        "    \n",
        "    ChatLogs = []\n",
        "    \n",
        "    async def is_about_element_that_we_record(self, sentence: str) -> SubjectsInMemoryStore:\n",
        "        \"\"\"\n",
        "        This function takes a sentenc and identify if there is element to record.\n",
        "        \n",
        "        We only record elements on subjects that we are allowed to.\n",
        "\n",
        "        Args:\n",
        "            sentence (str): a snippet of text from a conversation\n",
        "\n",
        "        Returns:\n",
        "            SubjectsInMemoryStore: In what category to store if we store something\n",
        "        \"\"\"\n",
        "        # UserName variable and value will be worwarded to the LLM\n",
        "        SpeakerName=self.MemoryElements.get(SubjectsInMemoryStore.PeopleNames, {}).get(self.CurrentUserId)\n",
        "        return await emulate()\n",
        "    \n",
        "    async def detect_action_type(self, sentence:str) -> ActionType:\n",
        "        \"\"\"\n",
        "        Identify the type of action that is implicitly requested in the sentence.\n",
        "        \n",
        "        The sentence is produced by us user of our system. Our system has a memory.\n",
        "        What action should be do on our memory to best answer user expectations?\n",
        "        \n",
        "        Return:\n",
        "            ActionType\n",
        "        \"\"\"\n",
        "        return await emulate() \n",
        "        \n",
        "    async def is_telling_who_he_is(self, sentence:str) -> bool:\n",
        "        \"\"\"\n",
        "        The speaker is telling about who he is in this sentence.\n",
        "\n",
        "        Args:\n",
        "            sentence (str): a sentence made by a speaker\n",
        "\n",
        "        Returns:\n",
        "            bool: True if we learn about his name or forstname\n",
        "        \"\"\"\n",
        "        return await emulate()\n",
        "    \n",
        "    async def fill_user_details(self, sentence:str, previouse_details:UserDetails)->UserDetails:\n",
        "        \"\"\"\n",
        "        Identifies data fields from the sentence\n",
        "\n",
        "        Args:\n",
        "            sentence (_type_): what the user say\n",
        "            previouse_details: What we new about this user\n",
        "            \n",
        "        Return:\n",
        "            Filled UserDetails object for this user\n",
        "        \"\"\"\n",
        "        return await emulate()\n",
        "    \n",
        "    \n",
        "    async def answer_to(self, sentence:str)->str:\n",
        "        \"\"\"\n",
        "        Proces the use input, handle memory, than answer.\n",
        "\n",
        "        Args:\n",
        "            sentence (str): what the client say\n",
        "\n",
        "        Returns:\n",
        "            str: what the agent answers\n",
        "        \"\"\"\n",
        "        if self.CurrentUserId is None:\n",
        "            tells_who_he_is = await self.is_telling_who_he_is(sentence)\n",
        "            print(tells_who_he_is)\n",
        "            if tells_who_he_is:\n",
        "                who = await self.fill_user_details(sentence, None)\n",
        "                self.CurrentUserId = \"you\"\n",
        "                if SubjectsInMemoryStore.PeopleNames not in self.MemoryElements:\n",
        "                    self.MemoryElements[SubjectsInMemoryStore.PeopleNames] = {}\n",
        "                self.MemoryElements.get(SubjectsInMemoryStore.PeopleNames)[self.CurrentUserId] = who\n",
        "                \n",
        "                return await self.format_answer(\"user name recoded\", who, sentence)\n",
        "            else:\n",
        "                return await self.format_answer(\"user shall identify first\", None, sentence)\n",
        "        else:\n",
        "            return await self.process_question(sentence)\n",
        "            \n",
        "        \n",
        "    async def find_first_element(self, subject:SubjectsInMemoryStore, question:str):\n",
        "        \"\"\"\n",
        "        Find who we are speaking aabout\n",
        "\n",
        "        Args:\n",
        "            question (str): question\n",
        "        \"\"\"\n",
        "        async def is_target(subject, question, key, value)->bool:\n",
        "            \"\"\"\n",
        "            Decide if the suject that is refered to by the question is the one described by key:value.\n",
        "            \n",
        "            If it is clear that we speak about this one, return True.\n",
        "            Otherwise return False.\n",
        "            \"\"\"\n",
        "            return await emulate()\n",
        "        \n",
        "        for k,v in self.MemoryElements.items():\n",
        "            print(f\"Look: {k}, {v}\")\n",
        "            if await is_target(subject, question, k, v):\n",
        "                print(\"FOUND: \", k, v)\n",
        "                return k, v\n",
        "            \n",
        "        return None, None\n",
        "            \n",
        "    async def format_answer(self, instruction, data, question)->str:\n",
        "        \"\"\"\n",
        "        Format a written answer to the question knowing that have executed `instruction` on 'data'.\n",
        "\n",
        "        Args:\n",
        "            instruction (_type_): what we have done or that we want to tell the user that we have done\n",
        "            data (_type_): the data found, inserted or modified\n",
        "            question (_type_): wht the used originally asked for.\n",
        "\n",
        "        Returns:\n",
        "            str: what we say to the user as an answer to his question\n",
        "        \"\"\"\n",
        "        return await emulate()\n",
        "        \n",
        "        \n",
        "    async def process_question(self, question):\n",
        "        \"\"\"\n",
        "        This is the main logic for thiw workflow agent\n",
        "\n",
        "        Args:\n",
        "            question (str): user question\n",
        "        \"\"\"\n",
        "        subject, action  = await asyncio.gather(\n",
        "            self.is_about_element_that_we_record(question),\n",
        "            self.detect_action_type(question)\n",
        "        )\n",
        "        print(subject, action)\n",
        "\n",
        "        if subject is SubjectsInMemoryStore.NotValidSubject:\n",
        "            return await self.format_answer(\"this suject is not handeled by this assistant\", action, question)\n",
        "\n",
        "        if action is ActionType.INSERT:\n",
        "            if subject is SubjectsInMemoryStore.PeopleNames:      \n",
        "                who_name, details = await asyncio.gather(\n",
        "                    closure(\"return a name for the person that we shall remember\")(question),\n",
        "                    self.fill_user_details(question, None)\n",
        "                )                \n",
        "                data = self.MemoryElements.get(SubjectsInMemoryStore.PeopleNames, {})[who_name] = details\n",
        "                return await self.format_answer(\"we have recorded the people\", data, question )\n",
        "                \n",
        "            else:\n",
        "                what_name, what_desc = await asyncio.gather(\n",
        "                    closure(\"return a name for the element that we shall remember\")(question),\n",
        "                    closure(\"return a description of the element that we shall remember\")(question)\n",
        "                )\n",
        "                if subject not in self.MemoryElements:\n",
        "                    self.MemoryElements[subject] = {}\n",
        "                self.MemoryElements.get(subject)[what_name] = what_desc\n",
        "                return await self.format_answer(\"we have recorded the element\", what_desc, question )\n",
        "                \n",
        "        elif action is ActionType.SELECT:\n",
        "            key, data = await self.find_first_element(subject, question)\n",
        "            \n",
        "            if key is None:\n",
        "                return await self.format_answer(\"we have not found the element in out knowledge\", None, question )\n",
        "                \n",
        "            print(\"FOUND: \", key, data)\n",
        "            return await self.format_answer(\"we have found the element in out knowledge\", {\"key\":key, \"data\":data}, question)\n",
        "            \n",
        "        else:\n",
        "            return await self.format_answer(\"action not yet supported\", action, question)\n",
        "            \n",
        "        \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "A=MyAgentWithMemory()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from OpenHosta.core.meta_prompt import EMULATE_META_PROMPT, USER_CALL_META_PROMPT\n",
        "from OpenHosta import MetaPrompt\n",
        " \n",
        "config.DefaultPipeline.emulate_meta_prompt = MetaPrompt(EMULATE_META_PROMPT.source)\n",
        "config.DefaultPipeline.emulate_meta_prompt.source+=\"/no_think\"\n",
        "\n",
        "config.DefaultPipeline.user_call_meta_prompt = MetaPrompt(USER_CALL_META_PROMPT.source)\n",
        "config.DefaultPipeline.user_call_meta_prompt.source+=\"\\n/no_think\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<class 'OpenHosta.core.meta_prompt.MetaPrompt'>\n",
              "MetaPrompt source:\n",
              "--------------------------------\n",
              "{% if variables_initialization %}# Values of parameters to be used\n",
              "{{ variables_initialization }}{% endif %}\n",
              "{{ function_name }}({{ function_call_arguments }})/no_think"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "config.DefaultPipeline.user_call_meta_prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "await A.is_telling_who_he_is(\"bonjour, je suis emmanuel\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SubjectsInMemoryStore.NotValidSubject ActionType.SELECT\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'D√©sol√©, je ne peux pas r√©pondre √† des questions sur la m√©t√©o.'"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "await A.answer_to(\"Quelle sera la m√©t√©o demain ?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "False\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'Please identify yourself first.'"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "await A.answer_to(\"Qui es-tu ?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "False\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'Please identify yourself first.'"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "await A.answer_to(\"Qui qui-je ?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'Nom m√©moris√© : Emmanuel Batt'"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "await A.answer_to(\"Je suis emmanuel batt. m√©morise cela \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{<SubjectsInMemoryStore.PeopleNames: 'PeopleNames'>: {'you': None}}"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "A.MemoryElements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SubjectsInMemoryStore.Grocery ActionType.INSERT\n"
          ]
        },
        {
          "ename": "CancelledError",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mCancelledError\u001b[39m                            Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m A.answer_to(\u001b[33m\"\u001b[39m\u001b[33mIl faut acheter du pain\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 115\u001b[39m, in \u001b[36mMyAgentWithMemory.answer_to\u001b[39m\u001b[34m(self, sentence)\u001b[39m\n\u001b[32m    113\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.format_answer(\u001b[33m\"\u001b[39m\u001b[33muser shall identify first\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m, sentence)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.process_question(sentence)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 190\u001b[39m, in \u001b[36mMyAgentWithMemory.process_question\u001b[39m\u001b[34m(self, question)\u001b[39m\n\u001b[32m    188\u001b[39m             \u001b[38;5;28mself\u001b[39m.MemoryElements[subject] = {}\n\u001b[32m    189\u001b[39m         \u001b[38;5;28mself\u001b[39m.MemoryElements.get(subject)[what_name] = what_desc\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.format_answer(\u001b[33m\"\u001b[39m\u001b[33mwe have recorded the element\u001b[39m\u001b[33m\"\u001b[39m, what_desc, question )\n\u001b[32m    192\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m action \u001b[38;5;129;01mis\u001b[39;00m ActionType.SELECT:\n\u001b[32m    193\u001b[39m     key, data = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.find_first_element(subject, question)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 154\u001b[39m, in \u001b[36mMyAgentWithMemory.format_answer\u001b[39m\u001b[34m(self, instruction, data, question)\u001b[39m\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mformat_answer\u001b[39m(\u001b[38;5;28mself\u001b[39m, instruction, data, question)->\u001b[38;5;28mstr\u001b[39m:\n\u001b[32m    143\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    144\u001b[39m \u001b[33;03m    Format a written answer to the question knowing that have executed `instruction` on 'data'.\u001b[39;00m\n\u001b[32m    145\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    152\u001b[39m \u001b[33;03m        str: what we say to the user as an answer to his question\u001b[39;00m\n\u001b[32m    153\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m emulate()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/VSCode_GitRepos/.venv/lib/python3.12/site-packages/OpenHosta/exec/emulate.py:74\u001b[39m, in \u001b[36memulate_async\u001b[39m\u001b[34m(pipeline, force_llm_args)\u001b[39m\n\u001b[32m     71\u001b[39m \u001b[38;5;66;03m# Convert the inspection to a prompt\u001b[39;00m\n\u001b[32m     72\u001b[39m messages = pipeline.push(inspection)\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m response_dict = \u001b[38;5;28;01mawait\u001b[39;00m inspection[\u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m].api_call_async(messages, pipeline.llm_args | force_llm_args)\n\u001b[32m     76\u001b[39m \u001b[38;5;66;03m# Convert the model response to a python object according to expected types\u001b[39;00m\n\u001b[32m     77\u001b[39m response_data = pipeline.pull(inspection, response_dict)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/VSCode_GitRepos/.venv/lib/python3.12/site-packages/OpenHosta/models/base_model.py:43\u001b[39m, in \u001b[36mModel.api_call_async\u001b[39m\u001b[34m(self, messages, llm_args)\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapi_call_async\u001b[39m(\n\u001b[32m     39\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     40\u001b[39m     messages: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]],\n\u001b[32m     41\u001b[39m     llm_args:\u001b[38;5;28mdict\u001b[39m = {}\n\u001b[32m     42\u001b[39m ) -> Dict:\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m     response_dict = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.get_event_loop().run_in_executor(\n\u001b[32m     44\u001b[39m             \u001b[38;5;28mself\u001b[39m.get_executor(),\n\u001b[32m     45\u001b[39m             \u001b[38;5;28mself\u001b[39m.api_call,\n\u001b[32m     46\u001b[39m             messages, llm_args\n\u001b[32m     47\u001b[39m         )\n\u001b[32m     48\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response_dict\n",
            "\u001b[31mCancelledError\u001b[39m: "
          ]
        }
      ],
      "source": [
        "await A.answer_to(\"Il faut acheter du pain\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SubjectsInMemoryStore.Grocery ActionType.SELECT\n",
            "Look: SubjectsInMemoryStore.PeopleNames, {'you': None}\n",
            "Look: SubjectsInMemoryStore.Grocery, {'pain': 'We need to buy bread'}\n",
            "FOUND:  SubjectsInMemoryStore.Grocery {'pain': 'We need to buy bread'}\n",
            "FOUND:  SubjectsInMemoryStore.Grocery {'pain': 'We need to buy bread'}\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'We need to buy bread'"
            ]
          },
          "execution_count": 148,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "await A.answer_to(\"Que faut il acheter ?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SubjectsInMemoryStore.Grocery ActionType.SELECT\n",
            "Look: SubjectsInMemoryStore.PeopleNames, {'you': None}\n",
            "Look: SubjectsInMemoryStore.Grocery, {'pain': 'We need to buy bread'}\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\"Je n'ai pas trouv√© d'information sur l'achat de l'eau.\""
            ]
          },
          "execution_count": 149,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "await A.answer_to(\"Faut il acheter de l'eau ?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SubjectsInMemoryStore.Grocery ActionType.SELECT\n",
            "Look: SubjectsInMemoryStore.PeopleNames, {'you': None}\n",
            "Look: SubjectsInMemoryStore.Grocery, {'pain': 'pain'}\n",
            "FOUND:  SubjectsInMemoryStore.Grocery {'pain': 'pain'}\n",
            "FOUND:  SubjectsInMemoryStore.Grocery {'pain': 'pain'}\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\"Non, pas besoin d'acheter du pain, il est d√©j√† dans nos connaissances.\""
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "await A.answer_to(\"Faut-il acheter du pain ?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "System prompt:\n",
            "-----------------\n",
            "You will act as a simulator for functions that cannot be implemented in actual code.\n",
            "\n",
            "I'll provide you with function definitions described in Python syntax. \n",
            "These functions will have no body and may even be impossible to implement in real code, \n",
            "so do not attempt to generate the implementation.\n",
            "\n",
            "Instead, imagine a realistic or reasonable output that matches the function description.\n",
            "I'll ask questions by directly writing out function calls as one would call them in Python.\n",
            "Respond with an appropriate return value, without adding any extra comments or explanations.\n",
            "If the provided information isn't enough to determine a clear answer, respond simply with \"None\".\n",
            "If assumptions need to be made, ensure they stay realistic, align with the provided description.\n",
            "\n",
            "Here's the function definition:\n",
            "\n",
            "```python\n",
            "<class 'str'>\n",
            "\n",
            "def format_answer(self, instruction, data, question) -> str:\n",
            "    \"\"\"\n",
            "        Format a written answer to the question knowing that have executed `instruction` on 'data'.\n",
            "\n",
            "        Args:\n",
            "            instruction (_type_): what we have done or that we want to tell the user that we have done\n",
            "            data (_type_): the data found, inserted or modified\n",
            "            question (_type_): wht the used originally asked for.\n",
            "\n",
            "        Returns:\n",
            "            str: what we say to the user as an answer to his question\n",
            "        \"\"\"\n",
            "\n",
            "    ...\n",
            "    ...behavior to be simulated...\n",
            "    ...\n",
            "\n",
            "    return ...appropriate return value...\n",
            "```\n",
            "\n",
            "/no_think\n",
            "User prompt:\n",
            "-----------------\n",
            "# Values of parameters to be used\n",
            "self=<__main__.MyAgentWithMemory object at 0x78d4e3f08350>\n",
            "\n",
            "instruction='we have found the element in out knowledge'\n",
            "\n",
            "data={'key': <SubjectsInMemoryStore.Grocery: 'Grocery'>, 'data': {'pain': 'pain'}}\n",
            "\n",
            "question='Faut-il acheter du pain ?'\n",
            "\n",
            "format_answer(self, instruction, data, question)/no_think\n",
            "LLM response:\n",
            "-----------------\n",
            "<think>\n",
            "Okay, let me try to figure this out. So, the user is asking for help with the format_answer function. The parameters given are:\n",
            "\n",
            "- self: a specific object instance.\n",
            "- instruction: 'we have found the element in out knowledge' (though 'out' should probably be 'our' but that's a typo maybe).\n",
            "- data: a dictionary with 'key' being a SubjectsInMemoryStore.Grocery and 'data' containing 'pain': 'pain'.\n",
            "- question: 'Faut-il acheter du pain ?' which translates to \"Should I buy bread?\"\n",
            "\n",
            "The function needs to return a string that's a proper answer to the question. Let's break this down.\n",
            "\n",
            "First, the question is asking if they should buy bread (\"Faut-il acheter du pain ?\"). The data shows that 'pain' (which means bread in French) is present in the data under 'data' as 'pain': 'pain'. Wait, the key is 'pain' and the value is 'pain'? Maybe it's a typo, but in French, 'pain' is bread. So the data indicates that 'pain' exists, meaning bread is available or is the item they're considering.\n",
            "\n",
            "The instruction says they've found the element in their knowledge. So the answer should probably confirm that bread (pain) is available, implying that they don't need to buy it, or maybe they should. Wait, if the data has 'pain', maybe it's already in their knowledge. Wait, the data structure is a bit confusing. The 'key' is the SubjectsInMemoryStore.Grocery (which is 'Grocery'), and 'data' is a dict with 'pain': 'pain'. So 'pain' is the item in the grocery store, and the value is 'pain'? Maybe it's a mistake, but I'll go with it.\n",
            "\n",
            "The question is a yes/no question: Should I buy bread? The data shows that pain (bread) is found, so maybe the answer is that they don't need to buy it because it's already available. Or maybe the 'data' represents the item they found, so the answer would be yes, you should buy it. Wait, the instruction says they found the element in their knowledge. So if the knowledge stores that bread is available, then the answer would be \"No, you don't need to buy bread because it's already available.\" Or maybe the data is indicating that they need to buy it because it's not in stock, but the data says 'pain': 'pain', which might mean it's present.\n",
            "\n",
            "Wait, the data's 'data' key has 'pain' as a key and 'pain' as the value. Maybe that's just an example. Let's assume that 'pain' (bread) is present in the data. So the answer should be something like \"Oui, il faut acheter du pain\" but that contradicts. Wait, no‚Äîif the data shows that 'pain' is found (available), then the answer would be that they don't need to buy it. Wait, but the question is \"Faut-il acheter du pain ?\", so if the data shows that pain is already available (like in their kitchen), then the answer would be \"Non, vous n'avez pas besoin d'acheter du pain car il est d√©j√† disponible.\"\n",
            "\n",
            "But the exact wording might vary. Let's see. The instruction says \"we have found the element in our knowledge\", so the system found that bread (pain) is in their knowledge. So the answer to \"Should I buy bread?\" would be no, because it's already there.\n",
            "\n",
            "Wait, but the question is phrased as a yes/no. The answer should probably confirm the status. So if the data has pain, then the answer is \"Non, vous n'avez pas besoin d'acheter du pain.\" But the exact French might be something else.\n",
            "\n",
            "Alternatively, maybe the data shows that 'pain' is the item they need to buy. Wait, the instruction says \"we have found the element in our knowledge\", so if the knowledge has the element (pain), then it's available, so you don't need to buy it. So the answer would be negative.\n",
            "\n",
            "So, putting it together, the answer should be a string in French that says \"Non, il n'est pas n√©cessaire d'acheter du pain car il est d√©j√† dans votre connaissance.\" But maybe simplified. The exact wording might be something like \"Non, vous n'avez pas besoin d'acheter du pain, nous l'avons d√©j√† trouv√© dans nos connaissances.\"\n",
            "\n",
            "Wait, but the instruction says \"we have found the element in our knowledge\", so the answer should reference that. So the response should inform the user that they've found the item (bread) in their knowledge, so they don't need to buy it.\n",
            "\n",
            "So, the answer could be \"Non, vous n'avez pas besoin d'acheter du pain car nous l'avons d√©j√† trouv√© dans nos connaissances.\"\n",
            "\n",
            "But maybe more natural in French. Let's check the translation. \"Faut-il acheter du pain ?\" is \"Should I buy bread?\". The data shows that pain (bread) is found, so the answer is \"Non, pas besoin d'acheter du pain car nous l'avons trouv√© dans nos connaissances.\"\n",
            "\n",
            "Wait, maybe shorter. Let's think of common responses. If something is already available, you say \"you don't need to buy it\". So the answer would be something like \"Non, vous n'avez pas besoin d'acheter du pain puisqu'il est d√©j√† disponible.\"\n",
            "\n",
            "But the function's task is to format the answer based on instruction, data, and question. The instruction is \"we have found the element in our knowledge\", so the answer should include that they found it, hence the user doesn't need to buy it.\n",
            "\n",
            "So the return string would be along the lines of \"Non, vous n'avez pas besoin d'acheter du pain car nous avons trouv√© l'information dans nos connaissances.\"\n",
            "\n",
            "But maybe more concise. Let's go with something like \"Non, pas besoin d'acheter du pain, il est d√©j√† dans nos connaissances.\" But using French properly.\n",
            "\n",
            "Wait, the data's 'data' key has 'pain': 'pain', so 'pain' is the item. So the answer should mention that 'pain' is found. So perhaps the answer is \"Non, vous n'avez pas besoin d'acheter du pain car il a √©t√© trouv√© dans nos connaissances.\"\n",
            "\n",
            "But maybe the exact wording the function should generate. Let's see the parameters again.\n",
            "\n",
            "Other possible error: the instruction says \"in out knowledge\" which is a typo for \"our knowledge\". So the correct instruction would be \"we have found the element in our knowledge\", so the answer should reflect that they found it.\n",
            "\n",
            "So the answer is \"Non, il n'est pas n√©cessaire d'acheter du pain car nous avons trouv√© l'√©l√©ment dans nos connaissances.\"\n",
            "\n",
            "But maybe more natural. Let's check standard French responses. For example, \"Non, vous n'avez pas besoin d'acheter du pain, il est d√©j√† disponible dans nos donn√©es.\"\n",
            "\n",
            "But given that the function's example uses the data key 'pain' as found, the answer should directly address the question. So \"Faut-il acheter du pain ?\" ‚Üí answer is no because it's found.\n",
            "\n",
            "Thus, the correct return string should be a French sentence saying no, you don't need to buy bread.\n",
            "\n",
            "Possible options:\n",
            "\n",
            "- \"Non, vous n'avez pas besoin d'acheter du pain car nous l'avons trouv√© dans nos connaissances.\"\n",
            "- \"Non, pas besoin d'acheter du pain, il est d√©j√† dans nos donn√©es.\"\n",
            "- \"Non, le pain a √©t√© trouv√© dans nos connaissances, pas besoin d'acheter.\"\n",
            "\n",
            "But the exact wording should match the parameters. Since the instruction says \"we have found the element\", the answer should reference that finding.\n",
            "\n",
            "So the best possible answer in French would be \"Non, vous n'avez pas besoin d'acheter du pain car nous avons d√©j√† trouv√© l'information dans nos connaissances.\"\n",
            "\n",
            "But maybe shorter, like \"Non, pas besoin d'acheter du pain, il est d√©j√† dans nos connaissances.\"\n",
            "\n",
            "I think that's reasonable. So the return value is that string.\n",
            "</think>\n",
            "\n",
            "Non, pas besoin d'acheter du pain, il est d√©j√† dans nos connaissances.\n"
          ]
        }
      ],
      "source": [
        "from OpenHosta import print_last_prompt\n",
        "print_last_prompt(A.format_answer)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNhsodrHnwmlJ7/0yYQTF1p",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "VSCode_GitRepos",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
