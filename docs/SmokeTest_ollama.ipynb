{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hand-e-fr/OpenHosta/blob/3.0.0_beta1/OpenHosta/tree/3.0.0_beta1/docs/SmokeTest_ollama.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Instal OpenHosta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2mUsing Python 3.12.3 environment at: /home/ebatt/VSCode_GitRepos/.venv\u001b[0m\n",
            "\u001b[2K\u001b[2mResolved \u001b[1m9 packages\u001b[0m \u001b[2min 151ms\u001b[0m\u001b[0m                                         \u001b[0m\n",
            "\u001b[2mAudited \u001b[1m9 packages\u001b[0m \u001b[2min 0.48ms\u001b[0m\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade -qqq uv\n",
        "\n",
        "# If you need to test a pre release uncomment:\n",
        "VERSION=\"@3.0.0_beta1\"\n",
        "\n",
        "!uv pip install -U \\\n",
        "    \"git+https://github.com/hand-e-fr/OpenHosta.git$VERSION\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3.0.0\n"
          ]
        }
      ],
      "source": [
        "import OpenHosta; print(OpenHosta.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Install Ollama\n",
        "\n",
        "This is to run a local model and have zero dependancies to externa API provides."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PfTzrCGKc8eG",
        "outputId": "31287fb2-e5a7-4c3f-d6e9-96a32de27d5f"
      },
      "outputs": [],
      "source": [
        "# This seems not to be accepted by google colab anymore. \n",
        "# Run it in a separtated terminal i refused\n",
        "#!apt install -y screen\n",
        "#!curl -fsSL https://ollama.com/install.sh | sh\n",
        "#!screen -dmS ollama ollama serve\n",
        "\n",
        "# So we just show how to install Ollama on linux\n",
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We test is ollama is available and how fast Qwen3 runs on it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "i1st6ZJpdGn0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total duration:       2.679660793s\n",
            "load duration:        2.002796383s\n",
            "prompt eval count:    9 token(s)\n",
            "prompt eval duration: 145.13192ms\n",
            "prompt eval rate:     62.01 tokens/s\n",
            "eval count:           116 token(s)\n",
            "eval duration:        529.879618ms\n",
            "eval rate:            218.92 tokens/s\n"
          ]
        }
      ],
      "source": [
        "!ollama run qwen3:4b hello --verbose  2>&1 | grep -E \":\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Use OpenHosta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "YJzVlcvjbf38"
      },
      "outputs": [],
      "source": [
        "\n",
        "from OpenHosta import config\n",
        "\n",
        "# You can replace with your own API (OpenAI chat/completion compatble)\n",
        "config.DefaultModel.base_url = \"http://localhost:11434/v1\"\n",
        "config.DefaultModel.model_name = \"qwen3:4b\"\n",
        "config.DefaultModel.api_key = \"not used by ollama local api\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**We test that the API is working with a simple call**\n",
        "\n",
        "`ask()` makes a very simple call to the API without adding any hidden prompt.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'<think>\\nOkay, the user said \"Hello World!\" and I need to respond. Let me think about how to approach this.\\n\\nFirst, \"Hello World!\" is a common greeting, so I should acknowledge it. Maybe start with a friendly reply. But what\\'s the best way to respond? Should I just say hello back, or add something else?\\n\\nWait, the user might be testing if I can handle simple interactions. Maybe I should respond in a way that shows I\\'m active and ready to help. Let me make sure the response is polite and open-ended. \\n\\nI should also check if there\\'s any specific context I\\'m missing. But since the user just said \"Hello World!\", it\\'s probably just a greeting. So a simple \"Hello!\" back should work. But maybe add a bit more to make it more engaging. For example, ask how they\\'re doing or offer assistance. \\n\\nHmm, but the user might not want a long response. Maybe keep it short and friendly. \"Hello! How can I assist you today?\" That sounds good. It\\'s concise and invites them to ask for help. \\n\\nAlternatively, maybe include an emoji to make it more personable. But I should be careful with that. The user didn\\'t specify, so maybe stick to text. \\n\\nYes, \"Hello! How can I assist you today?\" is appropriate. It\\'s polite, direct, and opens the door for further interaction. I don\\'t see any reason to add more. That should cover it.\\n</think>\\n\\nHello! How can I assist you today? üòä'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from OpenHosta import ask\n",
        "\n",
        "ask(\"Hello World!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Most OpenHosta functions are avaiale in sync and async flavor.\n",
        "\n",
        "Async versions are provided by adding `_async` or by importing from  `from OpenHosta.asynchrone`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 609
        },
        "id": "Kmw7wRVhcLzm",
        "outputId": "e60b6684-afaf-4f9e-b94e-eed9b4fbb303"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'<think>\\nOkay, the user said \"Hello World!\" so I need to respond appropriately. Since they greeted me, I should return a friendly greeting. Maybe say \"Hello!\" back and offer help. Keep it simple and welcoming. Let them know I\\'m here to assist with anything they need. No need for any other text, just a basic response.\\n</think>\\n\\nHello! How can I assist you today? üòä'"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from OpenHosta.asynchrone import ask\n",
        "\n",
        "# Another way to get the same result is:\n",
        "# from OpenHosta import ask_async as ask\n",
        "\n",
        "await ask(\"Hello World!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# The emulate function\n",
        "\n",
        "The main purpose of OpenHosta is to let you organize your code in python and let you decide later if you need to implement function bodies in native python or let an LLM do the job. \n",
        "\n",
        "This is really pythonic as types and doc strings will remain unchanged if you decide to replace `return emulate()` by your own code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "oNEhOe1ebtQ1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bonjour le monde!\n"
          ]
        }
      ],
      "source": [
        "from OpenHosta import emulate\n",
        "\n",
        "def translate(text:str, language:str)->str:\n",
        "    \"\"\"\n",
        "    This function translates the text in the ‚Äútext‚Äù parameter into the language specified in the ‚Äúlanguage‚Äù parameter.\n",
        "    \"\"\"\n",
        "    return emulate()\n",
        "\n",
        "result = translate(\"Hello World!\", \"French\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In very simple words, `emulate()` does the inspection of the function it is called from and buld a prompt that delegated computation of the function to an LLM.\n",
        "This is very usefull for NLP based functions, but also **smart** decisions in your workflow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [],
      "source": [
        "from enum import Enum\n",
        "\n",
        "class Choice(Enum):\n",
        "    \"\"\"\n",
        "    WebSearch = \"Generic Subject, but not enough confidence in LLM knowledge\"\n",
        "    UseLLM    = \"Generic Subject, obviouse answer.\"\n",
        "    UserRAG   = \"Company private subject, need to search for internal documentation through RAG\"\n",
        "    \"\"\"\n",
        "    WebSearch = \"WebSearch\"\n",
        "    UseLLM    = \"UseLLM\"\n",
        "    UserRAG   = \"UserRAG\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [],
      "source": [
        "from OpenHosta import emulate\n",
        "\n",
        "def decide_if_search_or_answer(query: str) -> Choice:\n",
        "    \"\"\"\n",
        "    Decide to search for answer elements in one of the multiple Choice locations.\n",
        "    \n",
        "    When the LLM is usure about the correct answer it shall query external data sources.\n",
        "    \n",
        "    Args:\n",
        "        subject (str): The question that we are working on\n",
        "    Return:\n",
        "        Choice: the best strategy to use\n",
        "    \"\"\"\n",
        "    return emulate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<Choice.UseLLM: 'UseLLM'>"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "decide_if_search_or_answer(\"april 2021 is after mars 2020?\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<Choice.WebSearch: 'WebSearch'>"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "decide_if_search_or_answer(\"who won the last NFL cup?\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<Choice.UseLLM: 'UseLLM'>"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "decide_if_search_or_answer(\"when was the first soccer world cup?\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ok, but can I see the prompt behind and what was really sent to the API?\n",
        "-YES-\n",
        "\n",
        "And the thinking and answer of the LLM ?\n",
        "-YES-"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "System prompt:\n",
            "-----------------\n",
            "You will act as a simulator for functions that cannot be implemented in actual code.\n",
            "\n",
            "I'll provide you with function definitions described in Python syntax. \n",
            "These functions will have no body and may even be impossible to implement in real code, \n",
            "so do not attempt to generate the implementation.\n",
            "\n",
            "Instead, imagine a realistic or reasonable output that matches the function description.\n",
            "I'll ask questions by directly writing out function calls as one would call them in Python.\n",
            "Respond with an appropriate return value, without adding any extra comments or explanations.\n",
            "If the provided information isn't enough to determine a clear answer, respond simply with \"None\".\n",
            "If assumptions need to be made, ensure they stay realistic, align with the provided description.\n",
            "\n",
            "Here's the function definition:\n",
            "\n",
            "```python\n",
            "# Python enum Choice definition.\n",
            "# When you return a Choice, print the enum member value as a string. I will identify the corresponding enum member.\n",
            "class Choice(Enum):\n",
            "    \"\"\"\n",
            "    WebSearch = \"Generic Subject, but not enough confidence in LLM knowledge\"\n",
            "    UseLLM    = \"Generic Subject, obviouse answer.\"\n",
            "    UserRAG   = \"Company private subject, need to search for internal documentation through RAG\"\n",
            "    \"\"\"\n",
            "    WebSearch = 'WebSearch'\n",
            "    UseLLM = 'UseLLM'\n",
            "    UserRAG = 'UserRAG'\n",
            "\n",
            "def decide_if_search_or_answer(query: str) -> Choice:\n",
            "    \"\"\"\n",
            "    Decide to search for answer elements in one of the multiple Choice locations.\n",
            "\n",
            "    When the LLM is usure about the correct answer it shall query external data sources.\n",
            "\n",
            "    Args:\n",
            "        subject (str): The question that we are working on\n",
            "    Return:\n",
            "        Choice: the best strategy to use\n",
            "    \"\"\"\n",
            "\n",
            "    ...\n",
            "    ...behavior to be simulated...\n",
            "    ...\n",
            "\n",
            "    return ...appropriate return value...\n",
            "```\n",
            "\n",
            "User prompt:\n",
            "-----------------\n",
            "# Values of parameters to be used\n",
            "query='when was the first soccer world cup?'\n",
            "\n",
            "decide_if_search_or_answer(query)\n",
            "LLM response:\n",
            "-----------------\n",
            "<think>\n",
            "Okay, let's see. The user is asking when the first soccer world cup was. I need to figure out whether the model should use WebSearch, UseLLM, or UserRAG.\n",
            "\n",
            "First, the question is about a specific historical event. The Soccer World Cup, also known as the FIFA World Cup, was first held in 1930. That's a well-known fact. But wait, the function's purpose is to decide whether to search, use the LLM, or use RAG. \n",
            "\n",
            "The description says that if the LLM is unsure, it should query external data. But if the answer is obvious, then UseLLM. However, the first soccer world cup is a commonly known fact. So maybe the LLM can answer it directly. But maybe the model isn't sure, so it might need to search. But the function is supposed to decide based on confidence. \n",
            "\n",
            "Alternatively, maybe the model is confident enough to answer without searching. So the answer would be UseLLM. But I need to check if the answer is trivial. Since the first World Cup was in 1930, which is a common knowledge, the model might not need to search. Therefore, the function would return UseLLM. \n",
            "\n",
            "Wait, but maybe the model isn't sure. For example, if the model is unsure about the exact year, it might need to search. But I think the correct answer is 1930. So the model can answer it directly. Therefore, return UseLLM. \n",
            "\n",
            "But the function's docstring says that UseLLM is for \"Generic Subject, obviouse answer.\" So if the answer is obvious, UseLLM. Since the first World Cup is a well-known event, the answer is obvious. Therefore, the function should return UseLLM.\n",
            "</think>\n",
            "\n",
            "UseLLM\n"
          ]
        }
      ],
      "source": [
        "from OpenHosta import print_last_prompt\n",
        "\n",
        "print_last_prompt(decide_if_search_or_answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Build a workflow agent\n",
        "\n",
        "Let's see how to impement an agent with memory within an object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import asyncio\n",
        "from dataclasses import dataclass\n",
        "from OpenHosta.asynchrone import emulate, closure\n",
        "\n",
        "from enum import Enum\n",
        "\n",
        "class SubjectsInMemoryStore(Enum):\n",
        "    FamilyMemberLinks = \"FamilyMemberLinks\"\n",
        "    PeopleNames = \"PeopleNames\"\n",
        "    Grocery = \"Grocery\"\n",
        "    CalendarEvents = \"CalendarEvents\"\n",
        "    NotValidSubject = \"NotValidSubject\"\n",
        "    \n",
        "class ActionType(Enum):\n",
        "    INSERT=\"INSERT\"\n",
        "    UPDATE=\"UPDATE\"\n",
        "    DELETE=\"DELETE\"\n",
        "    SELECT=\"SELECT\"\n",
        "    \n",
        "    \n",
        "@dataclass\n",
        "class UserDetails:\n",
        "    FirstName: str\n",
        "    LastName: str\n",
        "    Age: int\n",
        "\n",
        "class MyAgentWithMemory:\n",
        "    \"\"\"\n",
        "    This agent records elements from the conversation only if they are related to specific subkects\n",
        "    \"\"\"\n",
        "    \n",
        "    CurrentUserId:str = None\n",
        "    MemoryElements:dict = {}\n",
        "    \n",
        "    ChatLogs = []\n",
        "    \n",
        "    async def is_about_element_that_we_record(self, sentence: str) -> SubjectsInMemoryStore:\n",
        "        \"\"\"\n",
        "        This function takes a sentenc and identify if there is element to record.\n",
        "        \n",
        "        We only record elements on subjects that we are allowed to.\n",
        "\n",
        "        Args:\n",
        "            sentence (str): a snippet of text from a conversation\n",
        "\n",
        "        Returns:\n",
        "            SubjectsInMemoryStore: In what category to store if we store something\n",
        "        \"\"\"\n",
        "        # UserName variable and value will be worwarded to the LLM\n",
        "        SpeakerName=self.MemoryElements.get(SubjectsInMemoryStore.PeopleNames, {}).get(self.CurrentUserId)\n",
        "        return await emulate()\n",
        "    \n",
        "    async def detect_action_type(self, sentence:str) -> ActionType:\n",
        "        \"\"\"\n",
        "        Identify the type of action that is implicitly requested in the sentence.\n",
        "        \n",
        "        The sentence is produced by us user of our system. Our system has a memory.\n",
        "        What action should be do on our memory to best answer user expectations?\n",
        "        \n",
        "        Return:\n",
        "            ActionType\n",
        "        \"\"\"\n",
        "        return await emulate() \n",
        "        \n",
        "    async def is_telling_who_he_is(self, sentence:str) -> bool:\n",
        "        \"\"\"\n",
        "        The speaker is telling about who he is in this sentence.\n",
        "\n",
        "        Args:\n",
        "            sentence (str): a sentence made by a speaker\n",
        "\n",
        "        Returns:\n",
        "            bool: True if we learn about his name or forstname\n",
        "        \"\"\"\n",
        "        return await emulate()\n",
        "    \n",
        "    async def fill_user_details(self, sentence:str, previouse_details:UserDetails)->UserDetails:\n",
        "        \"\"\"\n",
        "        Identifies data fields from the sentence\n",
        "\n",
        "        Args:\n",
        "            sentence (_type_): what the user say\n",
        "            previouse_details: What we new about this user\n",
        "            \n",
        "        Return:\n",
        "            Filled UserDetails object for this user\n",
        "        \"\"\"\n",
        "        return await emulate()\n",
        "    \n",
        "    \n",
        "    async def answer_to(self, sentence:str)->str:\n",
        "        \"\"\"\n",
        "        Proces the use input, handle memory, than answer.\n",
        "\n",
        "        Args:\n",
        "            sentence (str): what the client say\n",
        "\n",
        "        Returns:\n",
        "            str: what the agent answers\n",
        "        \"\"\"\n",
        "        if self.CurrentUserId is None:\n",
        "            tells_who_he_is = await self.is_telling_who_he_is(sentence)\n",
        "            print(tells_who_he_is)\n",
        "            if tells_who_he_is:\n",
        "                who = await self.fill_user_details(sentence, None)\n",
        "                self.CurrentUserId = \"you\"\n",
        "                if SubjectsInMemoryStore.PeopleNames not in self.MemoryElements:\n",
        "                    self.MemoryElements[SubjectsInMemoryStore.PeopleNames] = {}\n",
        "                self.MemoryElements.get(SubjectsInMemoryStore.PeopleNames)[self.CurrentUserId] = who\n",
        "                \n",
        "                return await self.format_answer(\"user name recoded\", who, sentence)\n",
        "            else:\n",
        "                return await self.format_answer(\"user shall identify first\", None, sentence)\n",
        "        else:\n",
        "            return await self.process_question(sentence)\n",
        "            \n",
        "        \n",
        "    async def find_first_element(self, subject:SubjectsInMemoryStore, question:str):\n",
        "        \"\"\"\n",
        "        Find who we are speaking aabout\n",
        "\n",
        "        Args:\n",
        "            question (str): question\n",
        "        \"\"\"\n",
        "        async def is_target(subject, question, key, value)->bool:\n",
        "            \"\"\"\n",
        "            Decide if the suject that is refered to by the question is the one described by key:value.\n",
        "            \n",
        "            If it is clear that we speak about this one, return True.\n",
        "            Otherwise return False.\n",
        "            \"\"\"\n",
        "            return await emulate()\n",
        "        \n",
        "        for k,v in self.MemoryElements.items():\n",
        "            print(f\"Look: {k}, {v}\")\n",
        "            if await is_target(subject, question, k, v):\n",
        "                print(\"FOUND: \", k, v)\n",
        "                return k, v\n",
        "            \n",
        "        return None, None\n",
        "            \n",
        "    async def format_answer(self, instruction, data, question)->str:\n",
        "        \"\"\"\n",
        "        Format a written answer to the question knowing that have executed `instruction` on 'data'.\n",
        "\n",
        "        Args:\n",
        "            instruction (_type_): what we have done or that we want to tell the user that we have done\n",
        "            data (_type_): the data found, inserted or modified\n",
        "            question (_type_): wht the used originally asked for.\n",
        "\n",
        "        Returns:\n",
        "            str: what we say to the user as an answer to his question\n",
        "        \"\"\"\n",
        "        return await emulate()\n",
        "        \n",
        "        \n",
        "    async def process_question(self, question):\n",
        "        \"\"\"\n",
        "        This is the main logic for thiw workflow agent\n",
        "\n",
        "        Args:\n",
        "            question (str): user question\n",
        "        \"\"\"\n",
        "        subject, action  = await asyncio.gather(\n",
        "            self.is_about_element_that_we_record(question),\n",
        "            self.detect_action_type(question)\n",
        "        )\n",
        "        print(subject, action)\n",
        "\n",
        "        if subject is SubjectsInMemoryStore.NotValidSubject:\n",
        "            return await self.format_answer(\"this suject is not handeled by this assistant\", action, question)\n",
        "\n",
        "        if action is ActionType.INSERT:\n",
        "            if subject is SubjectsInMemoryStore.PeopleNames:      \n",
        "                who_name, details = await asyncio.gather(\n",
        "                    closure(\"return a name for the person that we shall remember\")(question),\n",
        "                    self.fill_user_details(question, None)\n",
        "                )                \n",
        "                data = self.MemoryElements.get(SubjectsInMemoryStore.PeopleNames, {})[who_name] = details\n",
        "                return await self.format_answer(\"we have recorded the people\", data, question )\n",
        "                \n",
        "            else:\n",
        "                what_name, what_desc = await asyncio.gather(\n",
        "                    closure(\"return a name for the element that we shall remember\")(question),\n",
        "                    closure(\"return a description of the element that we shall remember\")(question)\n",
        "                )\n",
        "                if subject not in self.MemoryElements:\n",
        "                    self.MemoryElements[subject] = {}\n",
        "                self.MemoryElements.get(subject)[what_name] = what_desc\n",
        "                return await self.format_answer(\"we have recorded the element\", what_desc, question )\n",
        "                \n",
        "        elif action is ActionType.SELECT:\n",
        "            key, data = await self.find_first_element(subject, question)\n",
        "            \n",
        "            if key is None:\n",
        "                return await self.format_answer(\"we have not found the element in out knowledge\", None, question )\n",
        "                \n",
        "            print(\"FOUND: \", key, data)\n",
        "            return await self.format_answer(\"we have found the element in out knowledge\", {\"key\":key, \"data\":data}, question)\n",
        "            \n",
        "        else:\n",
        "            return await self.format_answer(\"action not yet supported\", action, question)\n",
        "            \n",
        "        \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [],
      "source": [
        "A=MyAgentWithMemory()\n",
        "config.DefaultPipeline.emulate_meta_prompt.source += \"/no_think\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 84,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "await A.is_telling_who_he_is(\"bonjour, je suis emmanuel\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "False\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'The user shall identify first. Please provide the necessary information to answer the question about the weather tomorrow.'"
            ]
          },
          "execution_count": 85,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "await A.answer_to(\"Quelle sera la m√©t√©o demain ?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "False\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'Je suis un agent de suivi de la m√©moire.'"
            ]
          },
          "execution_count": 86,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "await A.answer_to(\"Qui es-tu ?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\"Je me nomme [Nom de l'utilisateur].\""
            ]
          },
          "execution_count": 87,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "await A.answer_to(\"Qui qui-je ?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SubjectsInMemoryStore.FamilyMemberLinks ActionType.INSERT\n",
            "Type detection failed for return a name for the element that we shall remember: 'None' is not a valid ArgType. Returning type 'Any'.\n",
            "Type detection failed for return a description of the element that we shall remember: 'None' is not a valid ArgType. Returning type 'Any'.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'what we say to the user as an answer to his question'"
            ]
          },
          "execution_count": 88,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "await A.answer_to(\"Je suis emmanuel batt. m√©morise cela \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{<SubjectsInMemoryStore.PeopleNames: 'PeopleNames'>: {'you': None},\n",
              " <SubjectsInMemoryStore.Grocery: 'Grocery'>: {'pain': 'Buy bread',\n",
              "  '\"ElementName\"': '\"Remembering the element: \\'Il faut acheter du pain\\'\"'},\n",
              " <SubjectsInMemoryStore.FamilyMemberLinks: 'FamilyMemberLinks'>: {\"'Je suis emmanuel batt. m√©morise cela '\": '\"Je suis emmanuel batt. m√©morise cela \"'}}"
            ]
          },
          "execution_count": 89,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "A.MemoryElements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SubjectsInMemoryStore.Grocery ActionType.INSERT\n",
            "Type detection failed for return a name for the element that we shall remember: 'None' is not a valid ArgType. Returning type 'Any'.\n",
            "Type detection failed for return a description of the element that we shall remember: 'None' is not a valid ArgType. Returning type 'Any'.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'what we say to the user as an answer to his question'"
            ]
          },
          "execution_count": 90,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "await A.answer_to(\"Il faut acheter du pain\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SubjectsInMemoryStore.Grocery ActionType.SELECT\n",
            "Look: SubjectsInMemoryStore.PeopleNames, {'you': None}\n",
            "Look: SubjectsInMemoryStore.Grocery, {'pain': 'Buy bread', '\"ElementName\"': '\"Remembering the element: \\'Il faut acheter du pain\\'\"', '\"element_name\"': '\"we shall remember the element \\'Il faut acheter du pain\\'\"'}\n",
            "Look: SubjectsInMemoryStore.FamilyMemberLinks, {\"'Je suis emmanuel batt. m√©morise cela '\": '\"Je suis emmanuel batt. m√©morise cela \"'}\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "cannot unpack non-iterable NoneType object",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[91]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m A.answer_to(\u001b[33m\"\u001b[39m\u001b[33mQue faut il acheter ?\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[58]\u001b[39m\u001b[32m, line 115\u001b[39m, in \u001b[36mMyAgentWithMemory.answer_to\u001b[39m\u001b[34m(self, sentence)\u001b[39m\n\u001b[32m    113\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.format_answer(\u001b[33m\"\u001b[39m\u001b[33muser shall identify first\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m, sentence)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.process_question(sentence)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[58]\u001b[39m\u001b[32m, line 193\u001b[39m, in \u001b[36mMyAgentWithMemory.process_question\u001b[39m\u001b[34m(self, question)\u001b[39m\n\u001b[32m    190\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.format_answer(\u001b[33m\"\u001b[39m\u001b[33mwe have recorded the element\u001b[39m\u001b[33m\"\u001b[39m, what_desc, question )\n\u001b[32m    192\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m action \u001b[38;5;129;01mis\u001b[39;00m ActionType.SELECT:\n\u001b[32m--> \u001b[39m\u001b[32m193\u001b[39m     key, data = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.find_first_element(subject, question)\n\u001b[32m    195\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    196\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.format_answer(\u001b[33m\"\u001b[39m\u001b[33mwe have not found the element in out knowledge\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m, question )\n",
            "\u001b[31mTypeError\u001b[39m: cannot unpack non-iterable NoneType object"
          ]
        }
      ],
      "source": [
        "await A.answer_to(\"Que faut il acheter ?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SubjectsInMemoryStore.Grocery ActionType.SELECT\n",
            "Look: SubjectsInMemoryStore.PeopleNames, {'you': None}\n",
            "Look: SubjectsInMemoryStore.Grocery, {'pain': 'Buy bread', '\"ElementName\"': '\"Remembering the element: \\'Il faut acheter du pain\\'\"', '\"element_name\"': '\"we shall remember the element \\'Il faut acheter du pain\\'\"'}\n",
            "Look: SubjectsInMemoryStore.FamilyMemberLinks, {\"'Je suis emmanuel batt. m√©morise cela '\": '\"Je suis emmanuel batt. m√©morise cela \"'}\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "cannot unpack non-iterable NoneType object",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[92]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m A.answer_to(\u001b[33m\"\u001b[39m\u001b[33mFaut il acheter de l\u001b[39m\u001b[33m'\u001b[39m\u001b[33meau ?\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[58]\u001b[39m\u001b[32m, line 115\u001b[39m, in \u001b[36mMyAgentWithMemory.answer_to\u001b[39m\u001b[34m(self, sentence)\u001b[39m\n\u001b[32m    113\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.format_answer(\u001b[33m\"\u001b[39m\u001b[33muser shall identify first\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m, sentence)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.process_question(sentence)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[58]\u001b[39m\u001b[32m, line 193\u001b[39m, in \u001b[36mMyAgentWithMemory.process_question\u001b[39m\u001b[34m(self, question)\u001b[39m\n\u001b[32m    190\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.format_answer(\u001b[33m\"\u001b[39m\u001b[33mwe have recorded the element\u001b[39m\u001b[33m\"\u001b[39m, what_desc, question )\n\u001b[32m    192\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m action \u001b[38;5;129;01mis\u001b[39;00m ActionType.SELECT:\n\u001b[32m--> \u001b[39m\u001b[32m193\u001b[39m     key, data = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.find_first_element(subject, question)\n\u001b[32m    195\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    196\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.format_answer(\u001b[33m\"\u001b[39m\u001b[33mwe have not found the element in out knowledge\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m, question )\n",
            "\u001b[31mTypeError\u001b[39m: cannot unpack non-iterable NoneType object"
          ]
        }
      ],
      "source": [
        "await A.answer_to(\"Faut il acheter de l'eau ?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "System prompt:\n",
            "-----------------\n",
            "You will act as a simulator for functions that cannot be implemented in actual code.\n",
            "\n",
            "I'll provide you with function definitions described in Python syntax. \n",
            "These functions will have no body and may even be impossible to implement in real code, \n",
            "so do not attempt to generate the implementation.\n",
            "\n",
            "Instead, imagine a realistic or reasonable output that matches the function description.\n",
            "I'll ask questions by directly writing out function calls as one would call them in Python.\n",
            "Respond with an appropriate return value, without adding any extra comments or explanations.\n",
            "If the provided information isn't enough to determine a clear answer, respond simply with \"None\".\n",
            "If assumptions need to be made, ensure they stay realistic, align with the provided description.\n",
            "\n",
            "Here's the function definition:\n",
            "\n",
            "```python\n",
            "<class 'str'>\n",
            "\n",
            "def format_answer(self, instruction, data, question) -> str:\n",
            "    \"\"\"\n",
            "        Format a written answer to the question knowing that have executed `instruction` on 'data'.\n",
            "\n",
            "        Args:\n",
            "            instruction (_type_): what we have done or that we want to tell the user that we have done\n",
            "            data (_type_): the data found, inserted or modified\n",
            "            question (_type_): wht the used originally asked for.\n",
            "\n",
            "        Returns:\n",
            "            str: what we say to the user as an answer to his question\n",
            "        \"\"\"\n",
            "\n",
            "    ...\n",
            "    ...behavior to be simulated...\n",
            "    ...\n",
            "\n",
            "    return ...appropriate return value...\n",
            "```\n",
            "\n",
            "User prompt:\n",
            "-----------------\n",
            "# Values of parameters to be used\n",
            "self=<__main__.MyAgentWithMemory object at 0x77039879e660>\n",
            "\n",
            "instruction='we have found the element in out knowledge'\n",
            "\n",
            "data={'key': <SubjectsInMemoryStore.Grocery: 'Grocery'>, 'data': {'pain': 'Buy bread'}}\n",
            "\n",
            "question=\"Faut il acheter de l'eau ?\"\n",
            "\n",
            "format_answer(self, instruction, data, question)\n",
            "LLM response:\n",
            "-----------------\n",
            "<think>\n",
            "Okay, let's see. The user is asking if they need to buy water, but the data provided has a key 'pain' with the value 'Buy bread'. The instruction says they found an element in their knowledge.\n",
            "\n",
            "Hmm, the function needs to format an answer based on the instruction and data. The data here is about 'pain' (maybe a typo for 'pain' as in 'pain' in French, but in the context of a grocery list, perhaps it's a key). The value is 'Buy bread', so the data indicates that bread needs to be bought. But the question is about water. \n",
            "\n",
            "Since the instruction mentions finding an element, maybe the answer should explain that the data found (buy bread) doesn't relate to water. So the answer would be that there's no information about water in the data provided. Therefore, the answer should state that based on the available data, there's no mention of water, so they shouldn't buy it unless other info is present.\n",
            "</think>\n",
            "\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "from OpenHosta import print_last_prompt\n",
        "print_last_prompt(A.format_answer)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNhsodrHnwmlJ7/0yYQTF1p",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "VSCode_GitRepos",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
