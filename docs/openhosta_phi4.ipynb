{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hand-e-fr/OpenHosta/blob/doc/docs/openhosta_phi4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OpenHosta basic example with a Local Phi-4 or gpt-4o\n",
        "\n",
        "This colab demonstrate simple use cases of OpenHosta. If you do not have an OpenAI (or other) API KEY, you can run the first part **Install a local Phi-4**. Otherwise, directly jump to step 2: **Basic Usage**."
      ],
      "metadata": {
        "id": "2ywSpiksruBs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install a local Phi-4"
      ],
      "metadata": {
        "id": "1DjZKsvksUPf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt install -y screen"
      ],
      "metadata": {
        "id": "5NCvoTi3phgq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb8b0174-c493-4636-cd27-301bef3556e4"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "screen is already the newest version (4.9.0-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ],
      "metadata": {
        "id": "dPQ1TFFGp4_w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78025a14-c681-4b7c-8fc4-9d6dbd546ce8"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Cleaning up old version at /usr/local/lib/ollama\n",
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "#########                                                                                      10.1%^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!screen -dmS ollama ollama serve"
      ],
      "metadata": {
        "id": "Rczlf_0Dp9b9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Can take 5 minutes\n",
        "!ollama run phi4 hello --verbose  2>&1 | grep -E \"([^0]0%|Bonjour|:)\""
      ],
      "metadata": {
        "id": "ivzgDLEuqEcI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "904161bf-1908-487a-d7d1-d94f3f826f43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pulling dde5aa3fc5ff...   0% ▕                ▏    0 B/2.0 GB                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff...   0% ▕                ▏    0 B/2.0 GB                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff...   0% ▕                ▏    0 B/2.0 GB                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff...   0% ▕                ▏ 334 KB/2.0 GB                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff...   0% ▕                ▏ 4.9 MB/2.0 GB                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff...  10% ▕█               ▏ 194 MB/2.0 GB  167 MB/s     10s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff...  10% ▕█               ▏ 200 MB/2.0 GB  167 MB/s     10s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff...  20% ▕███             ▏ 409 MB/2.0 GB  167 MB/s      9s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff...  30% ▕████            ▏ 599 MB/2.0 GB  241 MB/s      5s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff...  30% ▕████            ▏ 608 MB/2.0 GB  241 MB/s      5s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff...  40% ▕██████          ▏ 812 MB/2.0 GB  245 MB/s      4s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff...  70% ▕███████████     ▏ 1.4 GB/2.0 GB  248 MB/s      2s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff...  80% ▕████████████    ▏ 1.6 GB/2.0 GB  242 MB/s      1s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 966de95ca8a6...   0% ▕                ▏    0 B/1.4 KB                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 966de95ca8a6...   0% ▕                ▏    0 B/1.4 KB                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 966de95ca8a6...   0% ▕                ▏    0 B/1.4 KB                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling fcc5a6bec9da...   0% ▕                ▏    0 B/7.7 KB                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling fcc5a6bec9da...   0% ▕                ▏    0 B/7.7 KB                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling a70ff7e570d9...   0% ▕                ▏    0 B/6.0 KB                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling a70ff7e570d9...   0% ▕                ▏    0 B/6.0 KB                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling a70ff7e570d9...   0% ▕                ▏    0 B/6.0 KB                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 56bb8bd477a5...   0% ▕                ▏    0 B/  96 B                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 56bb8bd477a5...   0% ▕                ▏    0 B/  96 B                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 34bb5ab01051...   0% ▕                ▏    0 B/ 561 B                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "onr1w8jbEcby",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7ace8b2-3dcc-4597-c54f-3c1af0b08b23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting OpenHosta\n",
            "  Downloading OpenHosta-2.1.4-py3-none-any.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: requests>=2.32.3 in /usr/local/lib/python3.10/dist-packages (from OpenHosta) (2.32.3)\n",
            "Requirement already satisfied: typing_extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from OpenHosta) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.3->OpenHosta) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.3->OpenHosta) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.3->OpenHosta) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.3->OpenHosta) (2024.12.14)\n",
            "Downloading OpenHosta-2.1.4-py3-none-any.whl (60 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/60.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: OpenHosta\n",
            "Successfully installed OpenHosta-2.1.4\n"
          ]
        }
      ],
      "source": [
        "!pip install OpenHosta"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic Usage of OpenHosta"
      ],
      "metadata": {
        "id": "0x6AUM3osYK2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configure the LLM that you want to use"
      ],
      "metadata": {
        "id": "1ZEB71Ytsk4B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from OpenHosta import config\n",
        "\n",
        "# If you have an OpenAI API key you can use it like this:\n",
        "# Use default model (gpt-4o) through API key\n",
        "# config.set_default_apiKey(<<YOUR API KEY>>)\n",
        "\n",
        "# Comment this if you use OpenAI models\n",
        "# Use Microsoft local Phi-4 through ollama\n",
        "my_model=config.Model(\n",
        "    base_url=\"http://localhost:11434/v1/chat/completions\",\n",
        "    model=\"phi4\", api_key=\"none\", timeout=120\n",
        ")\n",
        "config.set_default_model(my_model)\n"
      ],
      "metadata": {
        "id": "SBC5-L0zqH1c"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Emulate functions using the seleted LLM"
      ],
      "metadata": {
        "id": "FauFveKWsp3G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from OpenHosta import emulate"
      ],
      "metadata": {
        "id": "SEpo1gfJT8Bg"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(text:str, language:str)->str:\n",
        "    \"\"\"\n",
        "    This function translates the text in the “text” parameter into the language specified in the “language” parameter.\n",
        "    \"\"\"\n",
        "    return emulate()\n",
        "\n",
        "result = translate(\"Hello World!\", \"French\")\n",
        "print(result)"
      ],
      "metadata": {
        "id": "gE1ecyDzUVFE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c98e267-7cbe-4c91-96de-281c644cc1c3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bonjour le monde!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# You can select another model like this\n",
        "#my_model = config.Model(\n",
        "#    model=\"gpt-4o-mini\",\n",
        "#    base_url=\"https://api.openai.com/v1/chat/completions\",\n",
        "#    api_key=<<API KEY>>\n",
        "#)"
      ],
      "metadata": {
        "id": "avGsfrxQUsCb"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_name_age(sentence:str, id:dict)->dict:\n",
        "    \"\"\"\n",
        "    This function find in a text the name and the age of a personn.\n",
        "\n",
        "    Args:\n",
        "        sentence: The text in which to search for information\n",
        "        id: The dictionary to fill in with information\n",
        "\n",
        "    Return:\n",
        "        A dictionary identical to the one passed in parameter, but filled with the information.\n",
        "        If the information is not found, fill with the values with “None”.\n",
        "    \"\"\"\n",
        "    return emulate(model=my_model)\n",
        "\n",
        "return_dict = {\"name\": \"\", \"age\": \"\"}\n",
        "result = find_name_age(\"Hello, I'm John Wick, i'm 30 and I live in New York\", return_dict)\n",
        "print(result)"
      ],
      "metadata": {
        "id": "I_AJQzoKU192",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c789fc11-5629-4b90-8c71-7a21bd66c44b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'name': 'John Wick', 'age': 30}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Specify advanced return types"
      ],
      "metadata": {
        "id": "bVuS-CC6szQM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Dict, Tuple, List\n",
        "\n",
        "def analyze_text(text: str) -> Dict[str, List[Tuple[int, str]]]:\n",
        "    \"\"\"Analyze text to map each word to a list of tuples containing word length and word.\"\"\"\n",
        "    return emulate()\n",
        "\n",
        "# Example usage\n",
        "analysis = analyze_text(\"Hello, World!\")\n",
        "\n",
        "print(analysis)\n",
        "print(type(analysis))"
      ],
      "metadata": {
        "id": "GXiKTdYm_GGp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd69c32d-6853-4ffe-a31a-5cdf526ad612"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'hello': [[5, 'hello']], 'world': [[5, 'world']]}\n",
            "<class 'dict'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Specify pydantic return strucures\n",
        "\n",
        "OpenHosta is compatible with pydantic. You can specify pydantic input and output types and OpenHosta will propagate schema and Field documentation to the LLM."
      ],
      "metadata": {
        "id": "R5qFAS06s7Eq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pydantic"
      ],
      "metadata": {
        "id": "-w2PyozHrklT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c38d6f8b-7e22-490a-a14d-65ccdfdec5bc"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (2.10.4)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.10/dist-packages (from pydantic) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic) (4.12.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel, Field\n",
        "\n",
        "class Personn(BaseModel):\n",
        "    name: str = Field(..., description = \"The full name\")\n",
        "    age: int\n",
        "\n",
        "def find_name_age_pydantic(sentence:str)->Personn:\n",
        "    \"\"\"\n",
        "    This function find in a text the name and the age of a personn.\n",
        "\n",
        "    Args:\n",
        "        sentence: The text in which to search for information\n",
        "\n",
        "    Return:\n",
        "        A Pydantic model, but filled with the information.\n",
        "        If the information is not found, fill with the values with “None”.\n",
        "    \"\"\"\n",
        "    return emulate()\n",
        "\n",
        "result = find_name_age_pydantic(\"Luke Skywalker is very surprising: he's only 27 when he becomes a Jedi.\")\n",
        "print(result)"
      ],
      "metadata": {
        "id": "wnn5ed4QWXYs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30fcd1de-8a47-4553-9407-4439f0332c96"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "name='Luke Skywalker' age=27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Limitations\n",
        "\n",
        "The emulation is limited by the LLM capabilities. Try to have it count r in strawberrry and you will get into troubles ;-).\n",
        "Make sure the LLM is capable and not alucinating before you implement an emulated function."
      ],
      "metadata": {
        "id": "VvcHBChjtyys"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_occurence_of_a_word(word :str, text: str) -> int:\n",
        "    \"\"\"\n",
        "    This function takes a word and a text and returns\n",
        "    the number of times the word appears in the text.\n",
        "    \"\"\"\n",
        "    return emulate()\n",
        "\n",
        "find_occurence_of_a_word(\"Hello\", \"Hello World Hello!\")\n"
      ],
      "metadata": {
        "id": "E5eTPiUyXjXR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8214322c-a41a-4abb-a915-f66c6ea20649"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    }
  ]
}