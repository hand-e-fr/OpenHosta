{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hand-e-fr/OpenHosta/blob/doc/docs/openhosta_agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OpenHosta Agent with GPT4\n",
        "\n",
        "This colab demonstrate simple use cases of OpenHosta. You need an OpenAI key to run it"
      ],
      "metadata": {
        "id": "2ywSpiksruBs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic Usage of AI Agents with OpenHosta"
      ],
      "metadata": {
        "id": "0x6AUM3osYK2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install OpenHosta"
      ],
      "metadata": {
        "id": "L9EUaVsR8x7x",
        "outputId": "cbd881be-3341-40f6-ab0e-efc61664de1b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting OpenHosta\n",
            "  Downloading OpenHosta-2.2.1-py3-none-any.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: requests>=2.32.3 in /usr/local/lib/python3.11/dist-packages (from OpenHosta) (2.32.3)\n",
            "Requirement already satisfied: typing_extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from OpenHosta) (4.12.2)\n",
            "Requirement already satisfied: jinja2>=3.1.5 in /usr/local/lib/python3.11/dist-packages (from OpenHosta) (3.1.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=3.1.5->OpenHosta) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.3->OpenHosta) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.3->OpenHosta) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.3->OpenHosta) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.3->OpenHosta) (2025.1.31)\n",
            "Downloading OpenHosta-2.2.1-py3-none-any.whl (61 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/61.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: OpenHosta\n",
            "Successfully installed OpenHosta-2.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configure the LLM that you want to use"
      ],
      "metadata": {
        "id": "1ZEB71Ytsk4B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from OpenHosta import config\n",
        "\n",
        "# You can skip this line if you plan to use the local ollama instace of phi-4\n",
        "config.set_default_apiKey(\"...\") # Ask me one through LinkedIn, I will provide one for testing purpose!"
      ],
      "metadata": {
        "id": "SBC5-L0zqH1c"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agent structure\n",
        "\n",
        "Ask a question then select best function to answer"
      ],
      "metadata": {
        "id": "T62JVKqU0rMg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Within Google colab you must use asynchrone version of ̀`emulate`. If you want to reproduce this without coroutines, remove `.asynchrone` in the import line and all `await` and `async`"
      ],
      "metadata": {
        "id": "7h9a4_BXBoS9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Emulate functions using the seleted LLM"
      ],
      "metadata": {
        "id": "FauFveKWsp3G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from OpenHosta.asynchrone import emulate"
      ],
      "metadata": {
        "id": "SEpo1gfJT8Bg"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mix of python and Openhosta functions that the agent will use\n",
        "async def add(a:float, b:float)->float:\n",
        "  \"\"\"\n",
        "  add two float numbers\n",
        "  \"\"\"\n",
        "  return a+b\n",
        "\n",
        "async def number_to_string(number:float)->str:\n",
        "  \"\"\"\n",
        "  Convert a number to a string that represent the number in letters\n",
        "  \"\"\"\n",
        "  return await emulate()\n",
        "\n",
        "async def string_to_number(input_string:str)->float:\n",
        "  \"\"\"\n",
        "  Convert input_string to a number\n",
        "  \"\"\"\n",
        "  return await emulate()"
      ],
      "metadata": {
        "id": "icgBCk9j00-2"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Agent main router\n",
        "from typing import Dict, Literal, List, Tuple\n",
        "\n",
        "Actions = {\n",
        "    \"add\": add,\n",
        "    \"number_to_string\": number_to_string,\n",
        "    \"string_to_number\": string_to_number\n",
        "}\n",
        "\n",
        "ActionType = Literal[\"add\", \"number_to_string\", \"string_to_number\", \"Done\"]\n"
      ],
      "metadata": {
        "id": "gZ02NiW_1ooR"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import inspect\n",
        "\n",
        "annotations = {f:inspect.get_annotations(f) for f in Actions.values()}"
      ],
      "metadata": {
        "id": "kHbkwB0EDfi7"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async def find_best_step_to_execute(stack:List[Tuple[ActionType, list, str]], actions:dict, target)->Tuple[ActionType, list]:\n",
        "  \"\"\"\n",
        "  Select the best action to get elements needed to achieve target.\n",
        "\n",
        "  You first print a strategy based on possible actions,\n",
        "  Then you look at already executed actions in stack,\n",
        "  Then you decide of the next action to take and its parameters\n",
        "\n",
        "  :param stack: a list of already taken actions with (ActionType, list of params, Action returned value)\n",
        "  :param actions: a dictionarry of possible tools\n",
        "  :param target: the overall target objective.\n",
        "\n",
        "  :return: The next action to take and its parameters. Return Done if there is no more action to take\n",
        "  \"\"\"\n",
        "  return await emulate()"
      ],
      "metadata": {
        "id": "Lxh9xIh0D3KZ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Agent main loop"
      ],
      "metadata": {
        "id": "bVuS-CC6szQM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Skip this part if you do not have OpenAI API Key"
      ],
      "metadata": {
        "id": "uxq5U8KEFyaf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "request = \"can you add twenty two and hundred thousant fifity five and print the result in letters\"\n",
        "\n",
        "stack=[]\n",
        "output = \"\"\n",
        "\n",
        "Done=False\n",
        "while not Done:\n",
        "  next_action, params = await find_best_step_to_execute(stack, annotations, request)\n",
        "  if next_action == \"Done\":\n",
        "    Done=True\n",
        "  else:\n",
        "    next_function = Actions[next_action]\n",
        "    print(f\"Executing {next_action} with params {params}\")\n",
        "    output = await next_function(*params)\n",
        "    stack.append([next_action, params, output])\n",
        "\n",
        "print(output)\n"
      ],
      "metadata": {
        "id": "GXiKTdYm_GGp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "outputId": "e1fad4aa-09ad-4603-991f-236dbf7b6f83"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RequestError",
          "evalue": "[OpenAICompatibleModel.api_call] Request failed:\n[OpenAICompatibleModel.api_call] Incorrect API key.\n\n\n\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mApiKeyError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/OpenHosta/models/OpenAICompatible.py\u001b[0m in \u001b[0;36mapi_call\u001b[0;34m(self, messages, json_output, llm_args)\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"invalid_api_key\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresponse_text\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mApiKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[OpenAICompatibleModel.api_call] Incorrect API key.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mApiKeyError\u001b[0m: [OpenAICompatibleModel.api_call] Incorrect API key.\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mRequestError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-54920ba90599>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mDone\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mDone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0mnext_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mfind_best_step_to_execute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannotations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mnext_action\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"Done\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mDone\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-a5c2965422ea>\u001b[0m in \u001b[0;36mfind_best_step_to_execute\u001b[0;34m(stack, actions, target)\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mnext\u001b[0m \u001b[0maction\u001b[0m \u001b[0mto\u001b[0m \u001b[0mtake\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mits\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mReturn\u001b[0m \u001b[0mDone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mthere\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mno\u001b[0m \u001b[0mmore\u001b[0m \u001b[0maction\u001b[0m \u001b[0mto\u001b[0m \u001b[0mtake\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m   \"\"\"\n\u001b[0;32m---> 15\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0memulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/OpenHosta/exec/emulate.py\u001b[0m in \u001b[0;36memulate_async\u001b[0;34m(model, prompt, use_locals_as_ctx, use_self_as_ctx, post_callback, llm_args)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \"\"\"\n\u001b[1;32m     73\u001b[0m     \u001b[0minspection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHostaInspector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m     return await _emulate(\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0minspection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minspection\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/OpenHosta/exec/emulate.py\u001b[0m in \u001b[0;36m_emulate\u001b[0;34m(inspection, model, prompt, use_locals_as_ctx, use_self_as_ctx, post_callback, llm_args)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         response_dict = await model.api_call_async([\n\u001b[0m\u001b[1;32m    117\u001b[0m                 \u001b[0;34m{\u001b[0m\u001b[0;34m\"role\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"system\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"content\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprompt_rendered\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m                 \u001b[0;34m{\u001b[0m\u001b[0;34m\"role\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"user\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"content\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprompt_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"PRE_FUNCTION_CALL\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/OpenHosta/models/OpenAICompatible.py\u001b[0m in \u001b[0;36mapi_call_async\u001b[0;34m(self, messages, json_output, llm_args)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mllm_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mdict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     ) -> Dict:\n\u001b[0;32m---> 58\u001b[0;31m         response_dict = await asyncio.get_event_loop().run_in_executor(\n\u001b[0m\u001b[1;32m     59\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_executor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_call\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/concurrent/futures/thread.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/OpenHosta/models/OpenAICompatible.py\u001b[0m in \u001b[0;36mapi_call\u001b[0;34m(self, messages, json_output, llm_args)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRequestError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[OpenAICompatibleModel.api_call] Request failed:\\n{e}\\n\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRequestError\u001b[0m: [OpenAICompatibleModel.api_call] Request failed:\n[OpenAICompatibleModel.api_call] Incorrect API key.\n\n\n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OpenHosta Agent with a Local Phi-4 LLM\n",
        "\n",
        "Redo the same but with a local phi-4 instance"
      ],
      "metadata": {
        "id": "URv0gDv_EzYk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt install -y screen\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "!screen -dmS ollama ollama serve\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oa1aifHkE65x",
        "outputId": "8e8c1811-a11b-425f-9a63-3e1dd8fb3330"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "Suggested packages:\n",
            "  byobu | screenie | iselect ncurses-term\n",
            "The following NEW packages will be installed:\n",
            "  screen\n",
            "0 upgraded, 1 newly installed, 0 to remove and 19 not upgraded.\n",
            "Need to get 672 kB of archives.\n",
            "After this operation, 1,029 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 screen amd64 4.9.0-1 [672 kB]\n",
            "Fetched 672 kB in 1s (596 kB/s)\n",
            "Selecting previously unselected package screen.\n",
            "(Reading database ... 124926 files and directories currently installed.)\n",
            "Preparing to unpack .../screen_4.9.0-1_amd64.deb ...\n",
            "Unpacking screen (4.9.0-1) ...\n",
            "Setting up screen (4.9.0-1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "############################################################################################# 100.0%\n",
            ">>> Creating ollama user...\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "VhM-MJDRKTo_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!echo Downloading model. This can take 5min ...\n",
        "!sleep 1\n",
        "#!ollama run phi4 hello --verbose  2>&1 | grep -E \":\"\n",
        "!ollama run qwen2.5-coder:3b hello --verbose  2>&1 | grep -E \":\""
      ],
      "metadata": {
        "id": "6OQi8D1HKUyL",
        "outputId": "5a0814e3-ce30-4867-bc53-2fb0d2733b47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading model. This can take 5min ...\n",
            "total duration:       3.638193238s\n",
            "load duration:        2.707758378s\n",
            "prompt eval count:    30 token(s)\n",
            "prompt eval duration: 302ms\n",
            "prompt eval rate:     99.34 tokens/s\n",
            "eval count:           37 token(s)\n",
            "eval duration:        627ms\n",
            "eval rate:            59.01 tokens/s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from OpenHosta import config\n",
        "# Use Microsoft local Phi-4 through ollama\n",
        "my_model=config.OpenAICompatibleModel(\n",
        "     base_url=\"http://localhost:11434/v1/chat/completions\",\n",
        "     #model=\"phi4\", api_key=\"none\", timeout=120\n",
        "     model=\"qwen2.5-coder:3b\", api_key=\"none\", timeout=120\n",
        " )\n",
        "config.set_default_model(my_model)\n"
      ],
      "metadata": {
        "id": "KYjUCTlXE9JN"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "request = \"can you add twenty two and hundred thousant fifity five and print the result in letters\"\n",
        "\n",
        "stack=[]\n",
        "output = \"\"\n",
        "\n",
        "Done=False\n",
        "while not Done:\n",
        "  next_action, params = await find_best_step_to_execute(stack, annotations, request)\n",
        "  if next_action == \"Done\":\n",
        "    Done=True\n",
        "  else:\n",
        "    next_function = Actions[next_action]\n",
        "    print(f\"Executing {next_action} with params {params}\")\n",
        "    output = await next_function(*params)\n",
        "    stack.append([next_action, params, output])\n",
        "\n",
        "print(output)\n"
      ],
      "metadata": {
        "id": "9DOhiLjVFrgT",
        "outputId": "79eeee6b-045c-40f2-f0dd-e5792b27e6af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "not enough values to unpack (expected 2, got 1)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-54920ba90599>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mDone\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mDone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0mnext_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mfind_best_step_to_execute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannotations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mnext_action\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"Done\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mDone\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
          ]
        }
      ]
    }
  ]
}